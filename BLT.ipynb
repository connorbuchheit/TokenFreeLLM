{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "block_size = 1024\n",
    "patch_size = 4  # Number of bytes per patch\n",
    "num_iters = 2000  # Number of training iterations\n",
    "learning_rate = 1e-3\n",
    "n_embd = 128      # Embedding dimension\n",
    "n_head = 2        # Number of attention heads\n",
    "n_layers_encoder = 2  # lE << lG # Number of layers in the encoder\n",
    "n_layers_latent = 4   # lG # Number of layers in the latent transformer\n",
    "n_layers_decoder = 2  # lD << lG # Number of layers in the decoder\n",
    "dropout = 0.1\n",
    "vocab_size = 256  # Number of unique bytes\n",
    "hash_sizes = range(3, 6)  # n-gram sizes\n",
    "hash_table_size = 50000  # Size of each hash table\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Custom Layer Normalization.\"\"\"\n",
    "    def __init__(self, ndim, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer Block with LayerNorm, Attention, and MLP.\"\"\"\n",
    "    def __init__(self, n_embd, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(n_embd)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=n_embd, num_heads=n_head, dropout=dropout, batch_first=True)\n",
    "        self.ln2 = LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(self.ln1(x), self.ln1(x), self.ln1(x))\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class CrossAttentionLayer(nn.Module):\n",
    "    \"\"\"Cross Attention Layer for Encoder and Decoder.\"\"\"\n",
    "    def __init__(self, query_dim, key_dim, n_head, dropout):\n",
    "        super().__init__()\n",
    "        self.ln_q = LayerNorm(query_dim)\n",
    "        self.ln_kv = LayerNorm(key_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=query_dim, num_heads=n_head, dropout=dropout, batch_first=True)\n",
    "        self.proj = nn.Linear(query_dim, query_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        query = self.ln_q(query)\n",
    "        key = self.ln_kv(key)\n",
    "        value = self.ln_kv(value)\n",
    "        attn_out, _ = self.attn(query, key, value)\n",
    "        attn_out = self.proj(attn_out)\n",
    "        attn_out = self.dropout(attn_out)\n",
    "        return query + attn_out\n",
    "\n",
    "class HashNGramEmbedding(nn.Module):\n",
    "    \"\"\"Hash n-gram Embeddings.\"\"\"\n",
    "    def __init__(self, hash_sizes, hash_table_size, n_embd):\n",
    "        super().__init__()\n",
    "        self.hash_sizes = hash_sizes\n",
    "        self.hash_table_size = hash_table_size\n",
    "        self.n_embd = n_embd\n",
    "        self.hash_embeddings = nn.ModuleDict({\n",
    "            f\"hash_{n}\": nn.Embedding(hash_table_size, n_embd) for n in hash_sizes\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        embeddings = torch.zeros(B, T, self.n_embd, device=x.device)\n",
    "        for n in self.hash_sizes:\n",
    "            if T < n:\n",
    "                continue\n",
    "            # Extract n-grams\n",
    "            ngrams = x.unfold(1, n, 1)  # [B, T - n +1, n]\n",
    "            # Compute hash\n",
    "            hashes = self.roll_poly_hash(ngrams)\n",
    "            hashes = hashes % self.hash_table_size\n",
    "            # Lookup embeddings\n",
    "            hash_emb = self.hash_embeddings[f\"hash_{n}\"](hashes)  # [B, T - n +1, n_embd]\n",
    "            # Scatter add\n",
    "            embeddings[:, n-1:T, :] += hash_emb\n",
    "        # Normalize\n",
    "        embeddings = embeddings / len(self.hash_sizes)\n",
    "        return embeddings  # [B, T, n_embd]\n",
    "\n",
    "    def roll_poly_hash(self, ngrams):\n",
    "        \"\"\"Simple polynomial rolling hash.\"\"\"\n",
    "        base = 257\n",
    "        hash_val = torch.zeros(ngrams.size(0), ngrams.size(1), device=ngrams.device, dtype=torch.long)\n",
    "        for i in range(ngrams.size(2)):\n",
    "            hash_val = (hash_val * base + ngrams[:, :, i].long()) % (2**32)\n",
    "        return hash_val\n",
    "\n",
    "class LocalEncoder(nn.Module):\n",
    "    \"\"\"Local Encoder that encodes input bytes into patch representations.\"\"\"\n",
    "    def __init__(self, vocab_size, n_embd, patch_size, hash_sizes, hash_table_size, n_head, dropout, lE):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_embd = n_embd\n",
    "        self.byte_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.hash_ngram = HashNGramEmbedding(hash_sizes, hash_table_size, n_embd)\n",
    "        self.transformer_blocks = nn.ModuleList([Block(n_embd, n_head, dropout) for _ in range(lE)])\n",
    "        self.cross_attn = CrossAttentionLayer(n_embd, n_embd, n_head, dropout)\n",
    "        self.ln = LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        # Byte Embedding\n",
    "        x_emb = self.byte_embedding(x)  # [B, T, C]\n",
    "        # Hash n-gram Embedding\n",
    "        hash_emb = self.hash_ngram(x)  # [B, T, C]\n",
    "        x_emb = x_emb + hash_emb  # [B, T, C]\n",
    "        # Transformer Layers\n",
    "        for block in self.transformer_blocks:\n",
    "            x_emb = block(x_emb)\n",
    "        # Cross-Attention to form patches\n",
    "        # Assume patches are non-overlapping\n",
    "        # Pad if necessary\n",
    "        if T % self.patch_size != 0:\n",
    "            pad_len = self.patch_size - (T % self.patch_size)\n",
    "            pad = torch.zeros((B, pad_len), dtype=x.dtype, device=x.device).long()\n",
    "            pad_emb = self.byte_embedding(pad)  # [B, pad_len, C]\n",
    "            pad_emb += self.hash_ngram(pad)  # Incorporate hash embeddings\n",
    "            x_emb = torch.cat([x_emb, pad_emb], dim=1)  # [B, T + pad_len, C]\n",
    "            T += pad_len\n",
    "        # Reshape and pool to create patch representations\n",
    "        patches = x_emb.view(B, T // self.patch_size, self.patch_size, self.n_embd).mean(dim=2)  # [B, N_patches, C]\n",
    "        patches = self.cross_attn(patches, x_emb, x_emb)  # [B, N_patches, C]\n",
    "        patches = self.ln(patches)\n",
    "        return patches  # [B, N_patches, C]\n",
    "\n",
    "class LatentTransformer(nn.Module):\n",
    "    \"\"\"Latent Transformer over patch representations.\"\"\"\n",
    "    def __init__(self, n_embd, n_head, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(n_embd, n_head, dropout) for _ in range(n_layers)])\n",
    "        self.ln_f = LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.ln_f(x)\n",
    "\n",
    "class LocalDecoder(nn.Module):\n",
    "    \"\"\"Local Decoder to decode the next patch of bytes.\"\"\"\n",
    "    def __init__(self, vocab_size, n_embd, patch_size, hash_sizes, hash_table_size, n_head, dropout, lD):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hash_ngram = HashNGramEmbedding(hash_sizes, hash_table_size, n_embd)\n",
    "        self.cross_attn = CrossAttentionLayer(n_embd, n_embd, n_head, dropout)\n",
    "        self.transformer_blocks = nn.ModuleList([Block(n_embd, n_head, dropout) for _ in range(lD)])\n",
    "        self.byte_proj = nn.Linear(n_embd, patch_size * vocab_size)\n",
    "        self.ln = LayerNorm(vocab_size)  # Corrected to vocab_size\n",
    "\n",
    "    def forward(self, x, patches):\n",
    "        # Cross-Attention: bytes query patches\n",
    "        x = self.cross_attn(x, patches, patches)  # [B, T_patches, C]\n",
    "        # Transformer Blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        # Project to bytes\n",
    "        x = self.byte_proj(x)  # [B, T_patches, patch_size * vocab_size]\n",
    "        # Reshape to [B, T, vocab_size]\n",
    "        B, T_patches, _ = x.shape\n",
    "        x = x.view(B, T_patches * self.patch_size, self.vocab_size)  # [B, T, vocab_size]\n",
    "        x = self.ln(x)  # [B, T, vocab_size]\n",
    "        return x  # [B, T, vocab_size]\n",
    "\n",
    "class ByteLatentTransformer(nn.Module):\n",
    "    \"\"\"Byte Latent Transformer combining encoder, transformer, and decoder.\"\"\"\n",
    "    def __init__(self, vocab_size, n_embd, n_head, n_layers_encoder, n_layers_latent, n_layers_decoder, dropout, patch_size, hash_sizes, hash_table_size, block_size):\n",
    "        super().__init__()\n",
    "        self.local_encoder = LocalEncoder(vocab_size, n_embd, patch_size, hash_sizes, hash_table_size, n_head, dropout, n_layers_encoder)\n",
    "        self.latent_transformer = LatentTransformer(n_embd, n_head, n_layers_latent, dropout)\n",
    "        self.local_decoder = LocalDecoder(vocab_size, n_embd, patch_size, hash_sizes, hash_table_size, n_head, dropout, n_layers_decoder)\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, block_size // patch_size, n_embd))\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        # Encode bytes to patches\n",
    "        patches = self.local_encoder(x)  # [B, N_patches, C]\n",
    "        # Add positional embeddings\n",
    "        patches = patches + self.pos_embedding[:, :patches.size(1), :]  # [B, N_patches, C]\n",
    "        # Transform patches\n",
    "        transformed_patches = self.latent_transformer(patches)  # [B, N_patches, C]\n",
    "        # Decode patches to bytes\n",
    "        logits = self.local_decoder(transformed_patches, patches)  # [B, T, vocab_size]\n",
    "        if targets is not None:\n",
    "            B, T, _ = logits.shape\n",
    "            loss = F.cross_entropy(logits.view(B * T, -1), targets.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "class PrepData:\n",
    "    \"\"\"Dataset handling byte-level text data.\"\"\"\n",
    "    def __init__(self, text, block_size, patch_size):\n",
    "        self.data = torch.frombuffer(text.encode('utf-8'), dtype=torch.uint8)\n",
    "        self.block_size = block_size\n",
    "        self.patch_size = patch_size\n",
    "        assert self.block_size % self.patch_size == 0, \"block_size must be divisible by patch_size.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size - self.patch_size + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.data[idx:idx + self.block_size + self.patch_size]\n",
    "        x = chunk[:self.block_size].long()\n",
    "        y = chunk[1:self.block_size + 1].long()  # Changed to next byte\n",
    "        return x, y\n",
    "\n",
    "def bits_per_byte(loss):\n",
    "    \"\"\"Calculate Bits-Per-Byte (BPB).\"\"\"\n",
    "    return loss / math.log(2)\n",
    "\n",
    "def train_model():\n",
    "    # Setup device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Load data\n",
    "    try:\n",
    "        with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'input.txt' not found. Please ensure the file exists in the 'data' directory.\")\n",
    "        return None, None\n",
    "\n",
    "    # Data statistics\n",
    "    raw_bytes = text.encode('utf-8')\n",
    "    n_bytes = len(raw_bytes)\n",
    "\n",
    "    # Create dataset and model\n",
    "    dataset = PrepData(text, block_size, patch_size)\n",
    "    model = ByteLatentTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        n_embd=n_embd,\n",
    "        n_head=n_head,\n",
    "        n_layers_encoder=n_layers_encoder,\n",
    "        n_layers_latent=n_layers_latent,\n",
    "        n_layers_decoder=n_layers_decoder,\n",
    "        dropout=dropout,\n",
    "        patch_size=patch_size,\n",
    "        hash_sizes=hash_sizes,\n",
    "        hash_table_size=hash_table_size,\n",
    "        block_size=block_size\n",
    "    ).to(device)\n",
    "\n",
    "    # Init positional embeddings\n",
    "    nn.init.normal_(model.pos_embedding, mean=0.0, std=0.02)\n",
    "\n",
    "    # Print hyperparameters\n",
    "    print(\"\\nHyperparameters:\")\n",
    "    print(f'n_embd: {n_embd}')\n",
    "    print(f'n_head: {n_head}')\n",
    "    print(f'n_layers_encoder: {n_layers_encoder}')\n",
    "    print(f'n_layers_latent: {n_layers_latent}')\n",
    "    print(f'n_layers_decoder: {n_layers_decoder}')\n",
    "    print(f'dropout: {dropout}')\n",
    "    print(f'vocab_size: {vocab_size}')\n",
    "    print(f'patch_size: {patch_size}')\n",
    "    print(f'Total bytes: {n_bytes:,}')\n",
    "    print(f'Block size: {block_size} bytes')\n",
    "    print(f'Batch size: {batch_size} sequences')\n",
    "    print(f'Number of iterations: {num_iters}')\n",
    "\n",
    "    # Init optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Init metric lists\n",
    "    loss_list = []\n",
    "    bpb_list = []\n",
    "    bytes_per_sec_list = []\n",
    "    patches_list = []\n",
    "\n",
    "    # Training loop\n",
    "    iter_num = 0\n",
    "    start_time = time.time()\n",
    "    total_loss = 0.0\n",
    "    print('\\nTraining...')\n",
    "    while iter_num < num_iters:\n",
    "        # Sample a batch\n",
    "        ix = torch.randint(len(dataset), (batch_size,))\n",
    "        x = torch.stack([dataset[i][0] for i in ix]).to(device)\n",
    "        y = torch.stack([dataset[i][1] for i in ix]).to(device)\n",
    "\n",
    "        # Ensure all indices are within [0, 255]\n",
    "        assert torch.all((x >= 0) & (x < vocab_size)), \"Byte indices out of range!\"\n",
    "        assert torch.all((y >= 0) & (y < vocab_size)), \"Byte indices out of range!\"\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits, loss = model(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Collect metrics\n",
    "        loss_list.append(loss.item())\n",
    "        current_bpb = bits_per_byte(total_loss / (iter_num + 1))\n",
    "        bpb_list.append(current_bpb)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        total_bytes = (iter_num + 1) * batch_size * block_size\n",
    "        bytes_per_sec = total_bytes / elapsed if elapsed > 0 else 0\n",
    "        bytes_per_sec_list.append(bytes_per_sec)\n",
    "\n",
    "        est_patches = total_bytes // patch_size\n",
    "        patches_list.append(est_patches)\n",
    "\n",
    "        # Reporting for every iteration]\n",
    "        print(f'step {iter_num +1}/{num_iters} | loss {loss.item():.4f} | '\n",
    "              f'{bytes_per_sec:.2f} bytes/s | bits-per-byte {current_bpb:.4f} | {est_patches} patches')\n",
    "\n",
    "        iter_num += 1\n",
    "\n",
    "    avg_time = time.time() - start_time\n",
    "    print(f'\\nTraining completed in {avg_time:.2f} seconds.')\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), \"blt_model.pt\")\n",
    "\n",
    "    # Downsample data for plotting if num_iters is large\n",
    "    plot_every = max(1, num_iters // 500)  # Adjust the divisor based on desired number of points\n",
    "    iterations = list(range(1, num_iters + 1, plot_every))\n",
    "    loss_plot = loss_list[::plot_every]\n",
    "    bpb_plot = bpb_list[::plot_every]\n",
    "    bytes_sec_plot = bytes_per_sec_list[::plot_every]\n",
    "    patches_plot = patches_list[::plot_every]\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(iterations, loss_plot, label='Loss', color='blue', linewidth=2)\n",
    "    plt.xlabel('Iterations', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.title('Loss / Iterations', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Bits-Per-Byte\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(iterations, bpb_plot, label='Bits-Per-Byte', color='orange', linewidth=2)\n",
    "    plt.xlabel('Iterations', fontsize=14)\n",
    "    plt.ylabel('Bits-Per-Byte', fontsize=14)\n",
    "    plt.title('Bits-Per-Byte / Iterations', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Bytes per Second\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.plot(iterations, bytes_sec_plot, label='Bytes/Sec', color='green', linewidth=2)\n",
    "    plt.xlabel('Iterations', fontsize=14)\n",
    "    plt.ylabel('Bytes per Second', fontsize=14)\n",
    "    plt.title('Training Speed / Iterations', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model, dataset\n",
    "\n",
    "def generate(model, prompt, max_new_bytes=500, temperature=1.0):\n",
    "    \"\"\"Generate text from the model given a prompt.\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    try:\n",
    "        x = torch.frombuffer(prompt.encode('utf-8'), dtype=torch.uint8).long().unsqueeze(0).to(device)\n",
    "    except UnicodeEncodeError:\n",
    "        print(\"Error: Prompt contains characters that cannot be encoded in UTF-8.\")\n",
    "        return \"\"\n",
    "\n",
    "    generated_bytes = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_bytes):\n",
    "            logits, _ = model(x)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_byte = torch.multinomial(probs, num_samples=1)\n",
    "            generated_bytes.append(next_byte.item())\n",
    "            x = torch.cat([x, next_byte.view(1, 1).to(device)], dim=1)\n",
    "\n",
    "    try:\n",
    "        generated_text = bytes(generated_bytes).decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        # Handle incomplete UTF-8 sequence\n",
    "        generated_text = bytes(generated_bytes[:-1]).decode('utf-8', errors='ignore')\n",
    "    return generated_text\n",
    "\n",
    "# Train model\n",
    "model, dataset = train_model()\n",
    "\n",
    "if model is not None:\n",
    "    # Sample from trained model\n",
    "    prompt = \"Once upon a\"\n",
    "    generated = generate(model, prompt)\n",
    "    print(f\"{prompt} {generated}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
